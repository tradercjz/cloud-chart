apiVersion: apps/v1
kind: Deployment
metadata:
  name: qwen-vllm
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: qwen-vllm
  template:
    metadata:
      labels:
        app: qwen-vllm
    spec:
      runtimeClassName: nvidia
      containers:
      - name: vllm
        image: vllm/vllm-openai:latest  # vLLM å®˜æ–¹é•œåƒ
        imagePullPolicy: IfNotPresent
        
        # å¯åŠ¨å‘½ä»¤ (å¯¹åº”ä½ çš„å‘½ä»¤è¡Œå‚æ•°)
        command: ["python3", "-m", "vllm.entrypoints.openai.api_server"]
        args:
        - "--model"
        - "/ssd/ssd6/hfmodels/qwen3full"
        - "--served-model-name"
        - "qwen-local"
        - "--port"
        - "8000"
        - "--gpu-memory-utilization"
        - "0.95"
        - "--dtype"
        - "bfloat16"
        - "--trust-remote-code"
        - "--max-model-len"
        - "131072"
        - "--port"
        - "9999"
        
        ports:
        - containerPort: 9999
        
        # ğŸŒŸ å…³é”®ç‚¹ 1: ç”³è¯· GPU èµ„æº
        resources:
          limits:
            nvidia.com/gpu: 1  # ç”³è¯· 1 å¼ å¡
        
        # ğŸŒŸ å…³é”®ç‚¹ 2: æŒ‚è½½å·
        volumeMounts:
        - name: model-cache
          mountPath: /ssd/ssd6/hfmodels/qwen3full # å®¹å™¨å†…æ¨¡å‹ç¼“å­˜è·¯å¾„
        - name: dshm
          mountPath: /dev/shm # å¢åŠ å…±äº«å†…å­˜ï¼ŒvLLM/PyTorch å¿…é¡»
          
      volumes:
      # æŒ‚è½½å®¿ä¸»æœºçš„æ¨¡å‹ç¼“å­˜ç›®å½•ï¼Œé¿å…é‡å¯ Pod é‡å¤ä¸‹è½½ 30GB æ¨¡å‹
      - name: model-cache
        hostPath:
          path: /ssd/ssd6/hfmodels/qwen3full  # å®¿ä¸»æœºä¸ŠåŸæœ¬å­˜æ¨¡å‹çš„åœ°æ–¹
          type: Directory
      # è§£å†³ PyTorch DataLoader éœ€è¦å¤§é‡å…±äº«å†…å­˜çš„é—®é¢˜
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 40Gi # æˆ–è€…æ›´å¤§

---
# Service: è®©é›†ç¾¤å†…å…¶ä»– Pod èƒ½é€šè¿‡ DNS è®¿é—®
apiVersion: v1
kind: Service
metadata:
  name: qwen-service # DNS åç§°
  namespace: default
spec:
  ports:
  - port: 9999
    targetPort: 9999
  selector:
    app: qwen-vllm